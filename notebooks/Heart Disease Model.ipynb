{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0e6ffd2",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction Model\n",
    "\n",
    "This project uses the UCI-Cleveland heart disease dataset to build machine learning models that can predict whether a patient has heart disease based on clinical measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb48831",
   "metadata": {},
   "source": [
    "**Problem Definition**\n",
    "\n",
    "Given patient attributes such as age, sex, chest pain type, cholesterol, resting ECG, and exercise-induced metrics, can we accurately classify whether the patient has heart disease?\n",
    "\n",
    "**Importance**\n",
    "\n",
    "Early identification of heart disease risk is critical in healthcare. Predictive models can support clinicians in screening and early diagnosis.\n",
    "\n",
    "**Goal**\n",
    "\n",
    "Train several ML models and compare their performance to identify the best classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c94719",
   "metadata": {},
   "source": [
    "## Import Libraries and Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c287d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_curve, auc, roc_auc_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110208c0",
   "metadata": {},
   "source": [
    "## Load and Inspect Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad883a2",
   "metadata": {},
   "source": [
    "### Loading Heart Disease Dataset\n",
    "\n",
    "**Dataset Source:**\n",
    "This project uses the UCI Cleveland Heart Disease dataset (collected from Kaggle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc495c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"heart_cleveland_upload.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ac3477",
   "metadata": {},
   "source": [
    "### Feature Description\n",
    "- **age** — Age in years  \n",
    "- **sex** — 1 = male, 0 = female  \n",
    "- **cp** — Chest pain type (0–3)  \n",
    "- **trestbps** — Resting blood pressure  \n",
    "- **chol** — Serum cholesterol  \n",
    "- **fbs** — Fasting blood sugar (1 = >120 mg/dl)  \n",
    "- **restecg** — Resting ECG results  \n",
    "- **thalach** — Maximum heart rate achieved  \n",
    "- **exang** — Exercise-induced angina (1 = yes)  \n",
    "- **oldpeak** — ST depression (exercise vs rest)  \n",
    "- **slope** — Slope of the ST segment  \n",
    "- **ca** — Number of major vessels (0–3)  \n",
    "- **thal** — Thallium stress test result  \n",
    "- **condition** — Target variable (1 = heart disease)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1103e8f",
   "metadata": {},
   "source": [
    "### Inspecting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce01d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe() #summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5f6419",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() #checking data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705085ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum() #checking for missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a1dcd1",
   "metadata": {},
   "source": [
    "## Visual and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ac9c92",
   "metadata": {},
   "source": [
    "### Age Distribution\n",
    "\n",
    "This histogram shows how patient ages are distributed. Overlaying the target classes helps us see whether certain age groups are more likely to have heart disease.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30faf946",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(data=df, x='age', hue='condition', kde=True, palette='Set2', alpha=0.6)\n",
    "plt.title('Age Distribution by Heart Disease Condition')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431550ca",
   "metadata": {},
   "source": [
    "**Insight:**  \n",
    "Patients with heart disease tend to be more common in the 45–65 age range.  \n",
    "Younger patients (below 40) show fewer cases of heart disease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c2c24c",
   "metadata": {},
   "source": [
    "### Chest Pain Type vs Heart Disease\n",
    "\n",
    "Chest pain (cp) is one of the strongest indicators in this dataset. We visualize how each chest pain type relates to heart disease cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4639ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.countplot(data=df, x='cp', hue='condition', palette='Set1')\n",
    "plt.title('Chest Pain Type vs Heart Disease')\n",
    "plt.xlabel('Chest Pain Type (cp)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f74826",
   "metadata": {},
   "source": [
    "**Insight:**  \n",
    "Chest pain type 2 and 3 show a higher number of heart disease cases compared to type 0.  \n",
    "This suggests chest pain is a strong predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0c6410",
   "metadata": {},
   "source": [
    "### Correlation Heatmap\n",
    "\n",
    "We calculate correlations between all numeric features to identify strong positive/negative relationships with heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837200f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap of Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3e619a",
   "metadata": {},
   "source": [
    "**Insight:**  \n",
    "- `thalach` (max heart rate) shows negative correlation with heart disease.  \n",
    "- `oldpeak` and `cp` show strong positive correlation.  \n",
    "- These features will likely be important for our models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc948212",
   "metadata": {},
   "source": [
    "## Preprocessing & Split Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f73b13c",
   "metadata": {},
   "source": [
    "**Feature Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2144d31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting all columns as features except Condition\n",
    "X = df.drop(columns=[\"condition\"])\n",
    "\n",
    "# Selecting Condition as Target\n",
    "y = df[\"condition\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f5a896",
   "metadata": {},
   "source": [
    "**Train-Test Split Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95392b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90224c5a",
   "metadata": {},
   "source": [
    "## Baseline Logistic Regression Model Train and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d258680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the pipeline\n",
    "baseline_logreg = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"logreg\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# training the dataset with baseline logistic regression\n",
    "baseline_logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ce55e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test data\n",
    "y_pred = baseline_logreg.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='binary', pos_label=1)\n",
    "recall = recall_score(y_test, y_pred, average='binary', pos_label=1)\n",
    "f1 = f1_score(y_test, y_pred, average='binary', pos_label=1)\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy:.4f}\")\n",
    "print(f\"Precision Score: {precision:.4f}\")\n",
    "print(f\"Recall Score: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4365a6",
   "metadata": {},
   "source": [
    "## Train and Evaluate Dataset with other models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de35553",
   "metadata": {},
   "source": [
    "### Define all base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812b0cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "# 1. LogisticRegression\n",
    "models[\"Logistic Regression\"] = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"logreg\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# 2. KNN Baseline\n",
    "models[\"KNN (Untuned)\"] = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"knn\", KNeighborsClassifier(n_neighbors=5))    \n",
    "])\n",
    "\n",
    "# 3. SVM(RBF)\n",
    "models[\"SVM (Untuned)\"] = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svc\", SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\"))\n",
    "])\n",
    "\n",
    "# 4. Decision Tree\n",
    "models[\"Decision Tree\"] = DecisionTreeClassifier(random_state=911)\n",
    "\n",
    "# 5. Random Forest\n",
    "models[\"Random Forest\"] = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da7fac8",
   "metadata": {},
   "source": [
    "### Define Tuned Models(KNN & SVM) Using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc89947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning Logistic Regression\n",
    "log_reg_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"logreg\", LogisticRegression())\n",
    "])\n",
    "\n",
    "param_grid_logreg = {\n",
    "    \"logreg__penalty\": [\"l1\", \"l2\"],\n",
    "    \"logreg__C\": [0.01, 0.1, 1, 10, 100],\n",
    "    \"logreg__solver\": [\"liblinear\"],        # supports both l1 and l2\n",
    "    \"logreg__class_weight\": [None, \"balanced\"],\n",
    "    \"logreg__max_iter\": [100, 200, 500, 1000],\n",
    "}\n",
    "\n",
    "grid_log_reg = GridSearchCV(\n",
    "    estimator=log_reg_pipe,\n",
    "    param_grid=param_grid_logreg,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_log_reg.fit(X_train, y_train)\n",
    "print(\"Best Logistic Regression Params in training: \", grid_log_reg.best_params_)\n",
    "\n",
    "best_log_reg = grid_log_reg.best_estimator_\n",
    "models[\"Logistic Regression (Tuned)\"] = best_log_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768c9236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning KNN \n",
    "knn_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"knn\", KNeighborsClassifier())    \n",
    "])\n",
    "\n",
    "param_grid_knn = {\n",
    "    \"knn__n_neighbors\": [3,5,7,11,15],\n",
    "    \"knn__weights\": [\"uniform\", \"distance\"]\n",
    "}\n",
    "\n",
    "grid_knn = GridSearchCV(\n",
    "    estimator = knn_pipe,\n",
    "    param_grid = param_grid_knn,\n",
    "    cv = 5, \n",
    "    scoring = \"accuracy\",\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "grid_knn.fit(X_train, y_train)\n",
    "print(\"Best KNN Params in training: \", grid_knn.best_params_)\n",
    "\n",
    "best_knn = grid_knn.best_estimator_\n",
    "models[\"KNN (Tuned)\"] = best_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6c3738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning SVM \n",
    "svm_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svc\", SVC())\n",
    "])\n",
    "\n",
    "param_grid_svm = {\n",
    "    \"svc__kernel\": [\"rbf\"],\n",
    "    \"svc__C\": [0.1, 1, 10],\n",
    "    \"svc__gamma\": [0.01, 0.1, 1.0]\n",
    "}\n",
    "\n",
    "grid_svm = GridSearchCV(\n",
    "    estimator = svm_pipe,\n",
    "    param_grid = param_grid_svm,\n",
    "    cv = 5, \n",
    "    scoring = \"accuracy\",\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "grid_svm.fit(X_train, y_train)\n",
    "print(\"Best SVM Params in training: \", grid_svm.best_params_)\n",
    "\n",
    "best_svm = grid_svm.best_estimator_\n",
    "models[\"SVM (Tuned)\"] = best_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe88224",
   "metadata": {},
   "source": [
    "### Evaluating All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d698d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to evaluate with all models\n",
    "def model_evaluation(name, model, X_train, y_train, X_test, y_test):\n",
    "    # train model\n",
    "    model.fit(X_train, y_train)\n",
    "    # predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"{name:30s} -> acc: {acc:.4f} | prec: {prec:.4f} | recall: {rec:.4f} | f1: {f1:.4f}\")\n",
    "\n",
    "    return acc, prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdbeb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating all models\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    acc, prec, rec, f1 = model_evaluation(name, model, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1 Score\": f1\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421ca0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with all evaluation metrics\n",
    "results_df = (\n",
    "    pd.DataFrame(results)\n",
    "    .sort_values(by=\"Accuracy\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af982fd6",
   "metadata": {},
   "source": [
    "## Final Model and Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37cba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = models[\"Logistic Regression\"]\n",
    "\n",
    "# refitting the final model\n",
    "final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e381b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the logistic regression model from the pipeline\n",
    "logreg = final_model.named_steps[\"logreg\"]\n",
    "\n",
    "# exctracting feature names\n",
    "feature_names = X.columns\n",
    "\n",
    "# accessing coefficients\n",
    "coefficients = logreg.coef_[0]\n",
    "\n",
    "# creating a dataframe\n",
    "coef_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Coefficient\": coefficients\n",
    "}).sort_values(by=\"Coefficient\", ascending=False)\n",
    "\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bd17e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting feature importance\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=coef_df, x=\"Coefficient\", y=\"Feature\", palette=\"viridis\")\n",
    "plt.title(\"Feature Importance (Logistic Regression Coefficients)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e99df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\", linewidth=3)\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve — Final Model — Logistic Regression\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"ROC AUC Score:\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb0f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary table \n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89596b54",
   "metadata": {},
   "source": [
    "### Final Model Conclusion\n",
    "\n",
    "**Chosen Model**: Logistic Regression (Untuned)\n",
    "\n",
    "**Reason for choosing the model**\n",
    "\n",
    "- Highest accuracy among all tested models\n",
    "\n",
    "- Best recall (important for detecting heart disease)\n",
    "\n",
    "- Interpretable coefficients\n",
    "\n",
    "- Stable and generalizable\n",
    "\n",
    "**Key Insights**\n",
    "\n",
    "- Positive coefficients (like oldpeak, cp) increase likelihood of heart disease\n",
    "\n",
    "- Negative coefficients (like thalach) lower disease probability\n",
    "\n",
    "- Age also plays a meaningful role\n",
    "\n",
    "**Model Performance**\n",
    "\n",
    "- Accuracy: ~92%\n",
    "\n",
    "- Recall: ~82% (very good for medical screening)\n",
    "\n",
    "- AUC: 0.953125\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "- Dataset is small (297 rows)\n",
    "\n",
    "- Missing lifestyle factors (smoking, weight, activity level)\n",
    "\n",
    "- Not a replacement for doctors\n",
    "\n",
    "- Should be validated on larger and more diverse populations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
